{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with BeautifulSoup and Requests Tutorial Series\n",
    "*This is the first part of a three parts tutorial series.*\n",
    "\n",
    "Scraping the web requires a little knowledge of HTML. Check out [this short crash course](https://www.youtube.com/watch?v=4K4QhIAfGKY) to learn the basics which is all you really need. Otherwise just follow along and you will understand everything just fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend you keep the [BeautifulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) open to understand better how it works and experiment on your own. We will also be using the famous [Requests library](https://2.python-requests.org/en/master/) so you might want to check that out as well. If you download this notebook and plan to run it locally, remember to install the required packages `pip install bs4 requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the web page we will be scraping\n",
    "laptop_page = 'https://www.elcorteingles.es/electronica/A27382739-portatil-lenovo-ideapad-330s-15ikb-i5-8-gb-256-gb-ssd'\n",
    "\n",
    "# Let's download the source HTML content of our selected product and print the status code\n",
    "r = requests.get(laptop_page)\n",
    "print(r.status_code)\n",
    "\n",
    "# Save the HTML only if the request status code is 200 (which means successful)\n",
    "if r.status_code == 200:\n",
    "    html = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have downloaded the source code, we need to understand how to navigate it. Open the web page, right click the title of the page (the laptop's name in this case) and select inspect element. You **don't have to select the entire title** in order to inspect it, I did it this way to make it easier to understand.\n",
    "\n",
    "![Inspect element](https://i.imgur.com/mKuWLJi.jpg)\n",
    "\n",
    "A new panel will open up where we can inspect the entire HTML source code but since we selected the title, it will go straight to that point upon opening. As you can see in the following image, the title is wrapped by **h1** tags which is what we will target in the next code block.\n",
    "\n",
    "![H1 tag](https://i.imgur.com/Dd6kiRc.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we convert the source html into a BeautifulSoup and store it in a variable called \"soup\" (by convention)\n",
    "# The soup object has a tree like structure we can easily navigate by targetting HTML tags\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Let's find the laptop's model by getting the <h1> tag. The find method returns a string,\n",
    "# if there was more than one \"h1\" tag, it would return only the first one it finds.\n",
    "title = soup.find('h1').text\n",
    "\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the model number. This one is a bit more tricky since is wrapped around __p__ tags which stands for 'paragraph'. There are usually many **p** tags in every web page but we are lucky that in this specific case, the tag we are targetting is also identified by a *class* name. In the next code block, we will learn how to target tags with classes.\n",
    "\n",
    "![](https://i.imgur.com/W1FWpmS.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract more valuable information like the model code. The find method can take multiple arguments,\n",
    "# the first argument is always the tag name and here we're adding \"class_\" to specify the class \"c12\"\n",
    "ugly_model_number = soup.find('p', class_='c12').text\n",
    "print(ugly_model_number)\n",
    "\n",
    "# We only want the code so we have to clean the string a little bit\n",
    "model_number = ugly_model_number.split()[-1]\n",
    "print(model_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about the price of the product?\n",
    "price = soup.find('span', class_='sale').text\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time for you to practice a little bit what we've learned so far. Extract the **color** and store it in the variable color. \\**Hint: you can only target one class at a time. HTML tags can have multiple classes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code below to extract the color\n",
    "color = soup.find('p', class_='variant-ctrl').text\n",
    "print(color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "There you have it, you have just learned the basics of web scraping with Python, BeautifulSoup and Requests. If you want to make more complex scripts, go ahead and dive right into part 2 of this tutorial series."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
